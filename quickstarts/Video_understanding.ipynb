{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BagaskaraGD/video-understanding/blob/main/quickstarts/Video_understanding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb5yiH5h8x3h"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "906e07f6e562"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMGdicu8PVD9"
      },
      "source": [
        "# Video understanding with Gemini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR4Ti6Q0QKIl"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Video_understanding.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w14yjWnPVD-"
      },
      "source": [
        "Gemini has from the begining been a multimodal model, capable of analyzing all sorts of medias using its [long context window](https://developers.googleblog.com/en/new-features-for-the-gemini-api-and-google-ai-studio/).\n",
        "\n",
        "[Gemini 2.0](https://ai.google.dev/gemini-api/docs/models/gemini-v2) and later bring video analysis to a whole new level as illustrated in [this video](https://www.youtube.com/watch?v=Mot-JEU26GQ):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CumMaR-sts53",
        "outputId": "5aa852b5-e941-4c95-a19c-02d661d7a50b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Mot-JEU26GQ?si=pcb7-_MZTSi_1Zkw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title Building with Gemini 2.0: Video understanding\n",
        "%%html\n",
        "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Mot-JEU26GQ?si=pcb7-_MZTSi_1Zkw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jexx9acnuDsA"
      },
      "source": [
        "This notebook will show you how to easily use Gemini to perform the same kind of video analysis. Each of them has different prompts that you can select using the dropdown, also feel free to experiment with your own.\n",
        "\n",
        "You can also check the [live demo](https://aistudio.google.com/starter-apps/video) and try it on your own videos on [AI Studio](https://aistudio.google.com/starter-apps/video)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0HWzIEAQYqz"
      },
      "source": [
        "## Setup\n",
        "\n",
        "This section install the SDK, set it up using your [API key](../quickstarts/Authentication.ipynb), imports the relevant libs, downloads the sample videos and upload them to Gemini.\n",
        "\n",
        "Expand the section if you are curious, but you can also just run it (it should take a couple of minutes since there are large files) and go straight to the examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzBKAaL4QYq0"
      },
      "source": [
        "### Install SDK\n",
        "\n",
        "The new **[Google Gen AI SDK](https://ai.google.dev/gemini-api/docs/sdks)** provides programmatic access to Gemini 2.0 (and previous models) using both the [Google AI for Developers](https://ai.google.dev/gemini-api/docs) and [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview) APIs. With a few exceptions, code that runs on one platform will run on both. This means that you can prototype an application using the Developer API and then migrate the application to Vertex AI without rewriting your code.\n",
        "\n",
        "More details about this new SDK on the [documentation](https://ai.google.dev/gemini-api/docs/sdks) or in the [Getting started](../quickstarts/Get_started.ipynb) notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IbKkL5ksQYq1"
      },
      "outputs": [],
      "source": [
        "%pip install -U -q 'google-genai'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDUGen_kQYq2"
      },
      "source": [
        "### Setup your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](../quickstarts/Authentication.ipynb) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0H_lRdlrQYq3"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3Lez1vBQYq3"
      },
      "source": [
        "### Initialize SDK client\n",
        "\n",
        "With the new SDK you now only need to initialize a client with you API key (or OAuth if using [Vertex AI](https://cloud.google.com/vertex-ai)). The model is now set in each call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "X3CAp9YrQYq4"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITgsQyaXQYq4"
      },
      "source": [
        "### Select the Gemini model\n",
        "\n",
        "Video understanding works best Gemini 2.5 pro model. You can also select former models to compare their behavior but it is recommended to use at least the 2.0 ones.\n",
        "\n",
        "For more information about all Gemini models, check the [documentation](https://ai.google.dev/gemini-api/docs/models/gemini) for extended information on each of them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "IO7IoqbrQYq5"
      },
      "outputs": [],
      "source": [
        "model_name = \"gemini-2.5-pro-preview-05-06\" # @param [\"gemini-1.5-flash-latest\",\"gemini-2.0-flash-lite\",\"gemini-2.0-flash\",\"gemini-2.5-flash-preview-04-17\",\"gemini-2.5-pro-exp-05-06\"] {\"allow-input\":true, isTemplate: true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv8ULT0lvJ47"
      },
      "source": [
        "### Get sample videos\n",
        "\n",
        "You will start with uploaded videos, as it's a more common use-case, but you will also see later that you can also use Youtube videos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "fMcwUw48vL1N"
      },
      "outputs": [],
      "source": [
        "# Load sample images\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/videos/Pottery.mp4 -O Pottery.mp4 -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/videos/Jukin_Trailcam_Videounderstanding.mp4 -O Trailcam.mp4 -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/videos/post_its.mp4 -O Post_its.mp4 -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/videos/user_study.mp4 -O User_study.mp4 -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4YMNQulz_yY"
      },
      "source": [
        "### Upload the videos\n",
        "\n",
        "Upload all the videos using the File API. You can find modre details about how to use it in the [Get Started](../quickstarts/Get_started.ipynb#scrollTo=KdUjkIQP-G_i) notebook.\n",
        "\n",
        "This can take a couple of minutes as the videos will need to be processed and tokenized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "LUUMJ4kE0OZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5187b484-9798-45a2-a733-342dae88764c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for video to be processed.\n",
            "Video processing complete: https://generativelanguage.googleapis.com/v1beta/files/tdr4qqob627u\n",
            "Waiting for video to be processed.\n",
            "Waiting for video to be processed.\n",
            "Video processing complete: https://generativelanguage.googleapis.com/v1beta/files/esy18ec6tyeh\n",
            "Waiting for video to be processed.\n",
            "Video processing complete: https://generativelanguage.googleapis.com/v1beta/files/me9lei1s377l\n",
            "Waiting for video to be processed.\n",
            "Video processing complete: https://generativelanguage.googleapis.com/v1beta/files/nwr00ggbeydk\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def upload_video(video_file_name):\n",
        "  video_file = client.files.upload(file=video_file_name)\n",
        "\n",
        "  while video_file.state == \"PROCESSING\":\n",
        "      print('Waiting for video to be processed.')\n",
        "      time.sleep(10)\n",
        "      video_file = client.files.get(name=video_file.name)\n",
        "\n",
        "  if video_file.state == \"FAILED\":\n",
        "    raise ValueError(video_file.state)\n",
        "  print(f'Video processing complete: ' + video_file.uri)\n",
        "\n",
        "  return video_file\n",
        "\n",
        "pottery_video = upload_video('Pottery.mp4')\n",
        "trailcam_video = upload_video('Trailcam.mp4')\n",
        "post_its_video = upload_video('Post_its.mp4')\n",
        "user_study_video = upload_video('User_study.mp4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF5tDbb-Q0oc"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "B0Z9QzC3Q2wX"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from PIL import Image\n",
        "from IPython.display import display, Markdown, HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAa7sCD7tuMW"
      },
      "source": [
        "# Search within videos\n",
        "\n",
        "First, try using the model to search within your videos and describe all the animal sightings in the trailcam video.\n",
        "\n",
        "<video controls width=\"500\"><source src=\"https://storage.googleapis.com/generativeai-downloads/videos/Jukin_Trailcam_Videounderstanding.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "PZw41-lsKKMf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "outputId": "9be580fe-623e-4477-d394-81953022a587"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```json\n[\n  {\"timecode\": \"00:00\", \"caption\": \"The video starts with a very blurry, close-up shot of an animal's fur, likely from bumping the camera.\"},\n  {\"timecode\": \"00:01\", \"caption\": \"The camera refocuses on a rocky, wooded area. A gray fox enters the frame from the right, sniffing the ground.\"},\n  {\"timecode\": \"00:04\", \"caption\": \"A second gray fox appears from the right, following the first. Both foxes forage on the ground.\"},\n  {\"timecode\": \"00:10\", \"caption\": \"One fox jumps onto a large rock in the center, while the other continues to sniff around on the ground.\"},\n  {\"timecode\": \"00:17\", \"caption\": \"The scene changes to infrared footage, likely at dusk or night. A cougar (mountain lion) is seen sniffing the ground in a leaf-littered wooded area.\"},\n  {\"timecode\": \"00:28\", \"caption\": \"The cougar lifts its head, looks around, then continues to sniff the ground before walking out of frame to the right.\"},\n  {\"timecode\": \"00:35\", \"caption\": \"Infrared footage shows two gray foxes in a dark, wooded area. One fox is rolling on the ground while the other watches.\"},\n  {\"timecode\": \"00:43\", \"caption\": \"The fox that was watching sits up, and then both foxes interact playfully before running off to the right.\"},\n  {\"timecode\": \"00:50\", \"caption\": \"The camera is briefly obscured by an animal bumping it. Infrared footage then shows two gray foxes in a rocky, wooded area. One is on a rock, the other on the ground.\"},\n  {\"timecode\": \"00:55\", \"caption\": \"The foxes move around, sniffing. One fox approaches and bumps the camera again, briefly obscuring the view before moving away.\"},\n  {\"timecode\": \"01:05\", \"caption\": \"Infrared footage shows a cougar (mountain lion) standing in a rocky, wooded area, looking around before walking off to the left.\"},\n  {\"timecode\": \"01:09\", \"caption\": \"A smaller cougar, possibly a cub, follows the first one, sniffing the ground as it moves.\"},\n  {\"timecode\": \"01:18\", \"caption\": \"Infrared footage shows a cougar (mountain lion) in a rocky, wooded area. Another, smaller cougar (cub) walks past it.\"},\n  {\"timecode\": \"01:24\", \"caption\": \"The larger cougar turns and walks towards the camera, then moves out of frame.\"},\n  {\"timecode\": \"01:29\", \"caption\": \"Infrared footage shows a bobcat standing in a dark, wooded area, looking around.\"},\n  {\"timecode\": \"01:32\", \"caption\": \"The bobcat sniffs the ground, moving slowly through the leaves, and then looks up alertly before continuing to forage.\"},\n  {\"timecode\": \"01:51\", \"caption\": \"Daytime footage shows a large black bear walking towards the camera in a sun-dappled forest, then turning and walking away.\"},\n  {\"timecode\": \"01:57\", \"caption\": \"Infrared footage (possibly dusk/dawn) shows a cougar (mountain lion) walking from right to left past the camera in a wooded area.\"},\n  {\"timecode\": \"02:05\", \"caption\": \"Daytime footage shows a very close-up view of a bear's fur as it bumps and moves past the camera.\"},\n  {\"timecode\": \"02:08\", \"caption\": \"The camera refocuses to show a bear cub walking away from the camera in a wooded area.\"},\n  {\"timecode\": \"02:11\", \"caption\": \"A second bear cub enters from the right, and both cubs forage on the ground.\"},\n  {\"timecode\": \"02:23\", \"caption\": \"Infrared footage shows a gray fox on a hillside, with city lights visible in the distant background. The fox sniffs the ground.\"},\n  {\"timecode\": \"02:35\", \"caption\": \"Infrared footage shows a black bear walking from left to right across the hillside, with city lights in the background.\"},\n  {\"timecode\": \"02:42\", \"caption\": \"Infrared footage shows a cougar (mountain lion) walking from left to right across the hillside, with city lights in the background.\"},\n  {\"timecode\": \"02:52\", \"caption\": \"Infrared footage shows a cougar (mountain lion) in a dark, wooded area, sniffing the base of a tree and the ground around it.\"},\n  {\"timecode\": \"03:05\", \"caption\": \"Daytime footage shows a black bear standing in a wooded area, looking around and sniffing the air. It scratches its side with its head.\"},\n  {\"timecode\": \"03:13\", \"caption\": \"The bear turns and looks in different directions, seemingly alert.\"},\n  {\"timecode\": \"03:22\", \"caption\": \"Daytime footage shows two black bears in a wooded area. One is closer to the camera, sniffing the ground, while the other is in the background.\"},\n  {\"timecode\": \"03:30\", \"caption\": \"The closer bear walks very near the camera. The other bear is visible behind it.\"},\n  {\"timecode\": \"03:38\", \"caption\": \"The bear closest to the camera turns and walks away, revealing the second bear more clearly. The second bear then sniffs the ground and walks towards where the first bear was.\"},\n  {\"timecode\": \"03:41\", \"caption\": \"Both bears are now foraging. A third, smaller bear (a cub) enters from the right and joins them.\"},\n  {\"timecode\": \"03:50\", \"caption\": \"The cub sits down and scratches itself while the two larger bears continue to forage.\"},\n  {\"timecode\": \"03:58\", \"caption\": \"The three bears move together, foraging on the ground.\"},\n  {\"timecode\": \"04:03\", \"caption\": \"Daytime footage shows two bear cubs. One is sniffing the ground while the other walks towards the camera.\"},\n  {\"timecode\": \"04:09\", \"caption\": \"A larger bear (mother) enters from the background and walks towards the cubs and the camera.\"},\n  {\"timecode\": \"04:22\", \"caption\": \"Infrared footage shows a bobcat sitting in a dark, wooded area near a fallen log. It looks around and then walks past the log.\"},\n  {\"timecode\": \"04:30\", \"caption\": \"Infrared footage shows a gray fox on a dark trail. It looks around alertly.\"},\n  {\"timecode\": \"04:36\", \"caption\": \"The fox becomes more alert, looks directly at the camera, then quickly turns and jumps away.\"},\n  {\"timecode\": \"04:50\", \"caption\": \"Infrared footage shows a gray fox walking away from the camera down a dark trail.\"},\n  {\"timecode\": \"04:57\", \"caption\": \"Infrared footage shows a cougar (mountain lion) in a dark, wooded area, sniffing the ground intently.\"},\n  {\"timecode\": \"05:04\", \"caption\": \"The cougar looks up briefly, then continues sniffing before walking slowly out of frame to the right.\"}\n]\n```"
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "prompt = \"For each scene in this video, generate captions that describe the scene along with any spoken text placed in quotation marks. Place each caption into an object with the timecode of the caption in the video.\"  # @param [\"For each scene in this video, generate captions that describe the scene along with any spoken text placed in quotation marks. Place each caption into an object with the timecode of the caption in the video.\", \"Organize all scenes from this video in a table, along with timecode, a short description, a list of objects visible in the scene (with representative emojis) and an estimation of the level of excitement on a scale of 1 to 10\"] {\"allow-input\":true}\n",
        "\n",
        "video = trailcam_video # @param [\"trailcam_video\", \"pottery_video\", \"post_its_video\", \"user_study_video\"] {\"type\":\"raw\",\"allow-input\":true}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=model_name,\n",
        "    contents=[\n",
        "        video,\n",
        "        prompt,\n",
        "    ]\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOQzKYGJKAnD"
      },
      "source": [
        "The prompt used is quite a generic one, but you can get even better results if you cutomize it to your needs (like asking specifically for foxes).\n",
        "\n",
        "The [live demo on AI Studio](https://aistudio.google.com/starter-apps/video) shows how you can postprocess this output to jump directly to the the specific part of the video by clicking on the timecodes. If you are interested, you can check the [code of that demo on Github](https://github.com/google-gemini/starter-applets/tree/main/video)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wog32E7CKnT6"
      },
      "source": [
        "# Extract and organize text\n",
        "\n",
        "Gemini can also read what's in the video and extract it in an organized way. You can even use Gemini reasoning capabilities to generate new ideas for you.\n",
        "\n",
        "<video controls width=\"400\"><source src=\"https://storage.googleapis.com/generativeai-downloads/videos/post_its.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "baNCeA3GKrfu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "774b9679-96fd-4463-a361-8ae088e59a0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, here are the transcribed project names from the sticky notes, organized into a table, followed by a few new ideas based on the apparent themes.\n\nThe whiteboard heading says: \"Brainstorm: Project Name\"\n\nHere are the names from the sticky notes:\n\n| Project Name Ideas (from sticky notes) |\n| :------------------------------------ |\n| Aether                               |\n| Andromeda's Reach                    |\n| Astral Forge                         |\n| Athena                               |\n| Athena's Eye                         |\n| Bayes Theorem (written as \"Theroem\") |\n| Canis Major                          |\n| Centaurus                            |\n| Celestial Drift                      |\n| Cerberus                             |\n| Chaos Field                          |\n| Chaos Theory (appears twice)         |\n| Chimera Dream                        |\n| Comet's Tail                         |\n| Convergence                          |\n| Delphinus                            |\n| Draco                                |\n| Echo                                 |\n| Equilibrium                          |\n| Euler's Path                         |\n| Fractal                              |\n| Galactic Core                        |\n| Golden Ratio                         |\n| Hera                                 |\n| Infinity Loop                        |\n| Leo Minor                            |\n| Lunar Eclipse                        |\n| Lynx                                 |\n| Lyra                                 |\n| Medusa                               |\n| Odin                                 |\n| Orion's Belt                         |\n| Orion's Sword                        |\n| Pandora's Box                        |\n| Persius Shield                       |\n| Phoenix                              |\n| Prometheus Rising                    |\n| Riemann's Hypothesis                 |\n| Sagitta                              |\n| Serpens                              |\n| Stellar Nexus                        |\n| Stokes Theorem                       |\n| Supernova Echo                       |\n| Symmetry                             |\n| Taylor Series                        |\n| Titan                                |\n| Vector                               |\n| Zephyr                               |\n\n---\n\nLooking at the themes (mythology, astronomy, mathematics/science concepts, evocative words), here are a few more ideas:\n\n**New Project Name Ideas:**\n\n1.  **Nebula Prime:** (Astronomy + Primacy)\n2.  **Quantum Leap:** (Physics + Progress)\n3.  **Valkyrie Core:** (Mythology + Centrality/Technology)\n4.  **Axiom Engine:** (Logic/Math + Power/Mechanism)\n5.  **Cosmic Cipher:** (Astronomy + Mystery/Code)\n6.  **Helios Construct:** (Mythology + Creation/Build)\n7.  **Event Horizon:** (Astronomy/Physics - a point of no return, or a significant boundary)\n8.  **Janus Protocol:** (Mythology - god of beginnings, gates, transitions + Order/Method)\n9.  **Singularity Drive:** (Physics/Tech + Propulsion/Core)\n10. **Orion's Codex:** (Astronomy/Mythology + Knowledge/System)"
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "prompt = \"Transcribe the sticky notes, organize them and put it in a table. Can you come up with a few more ideas?\" # @param [\"Transcribe the sticky notes, organize them and put it in a table. Can you come up with a few more ideas?\", \"Which of those names who fit an AI product that can resolve complex questions using its thinking abilities?\"] {\"allow-input\":true}\n",
        "\n",
        "video = post_its_video # @param [\"trailcam_video\", \"pottery_video\", \"post_its_video\", \"user_study_video\"] {\"type\":\"raw\",\"allow-input\":true}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=model_name,\n",
        "    contents=[\n",
        "        video,\n",
        "        prompt,\n",
        "    ]\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjKIsLDMTNk1"
      },
      "source": [
        "# Structure information\n",
        "\n",
        "Gemini 2.0 is not only able to read text but also to reason and structure about real world objects. Like in this video about a display of ceramics with handwritten prices and notes.\n",
        "\n",
        "<video controls width=\"500\"><source src=\"https://storage.googleapis.com/generativeai-downloads/videos/Pottery.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqzqedMFT5Wp",
        "outputId": "8ceab610-11ae-4ba5-9d61-72b1eab2d9b3"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "Okay, here is a table summarizing the items and notes shown in the image:\n\n| Item          | Description / Notes                       | Price |\n| :------------ | :---------------------------------------- | :---- |\n| Tumblers      | Glaze: #5 Artichoke double dip<br>4\"h x 3\"d (-ish) | \\$20  |\n| Small bowls   | 3.5\"h x 6.5\"d                             | \\$35  |\n| Med bowls     | 4\"h x 7\"d                                 | \\$40  |\n| *Glaze Info*  | #5 Artichoke double dip (Test tile shown) | N/A   |\n| *Glaze Info*  | #6 Gemini double dip, SLOW COOL (Test tile shown, marked 6rb) | N/A   |\n\n**Note:** The glaze for the small and medium bowls appears to be the \"#6 Gemini double dip\" based on visual similarity to the test tile, although the notes next to the bowls don't explicitly state the glaze name.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"Give me a table of my items and notes\" # @param [\"Give me a table of my items and notes\", \"Help me come up with a selling pitch for my potteries\"] {\"allow-input\":true}\n",
        "\n",
        "video = pottery_video # @param [\"trailcam_video\", \"pottery_video\", \"post_its_video\", \"user_study_video\"] {\"type\":\"raw\",\"allow-input\":true}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=model_name,\n",
        "    contents=[\n",
        "        video,\n",
        "        prompt,\n",
        "    ],\n",
        "    config = types.GenerateContentConfig(\n",
        "        system_instruction=\"Don't forget to escape the dollar signs\",\n",
        "    )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsh6i-Z6VHNK"
      },
      "source": [
        "As you can see, Gemini is able to grasp to with item corresponds each note, including the last one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIfsFC0pVUTD"
      },
      "source": [
        "# Analyze screen recordings for key moments\n",
        "\n",
        "You can also use the model to analyze screen recordings. Let's say you're doing user studies on how people use your product, so you end up with lots of screen recordings, like this one, that you have to manually comb through.\n",
        "With just one prompt, the model can describe all the actions in your video.\n",
        "\n",
        "<video controls width=\"400\"><source src=\"https://storage.googleapis.com/generativeai-downloads/videos/user_study.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrMHZ0MxW75y",
        "outputId": "7b98e818-0707-4c7f-ee86-0347b7713fbc"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "Here is a summary of the video:\n\n(00:00-00:10) The video displays a mobile application called \"My Garden App\" showcasing various plants available for purchase.\n(00:10-00:17) The user interacts with the app by clicking the \"Like\" button for the Rose Plant, Fern, and Cactus, turning the buttons red.\n(00:13-00:25) They proceed to add the Fern, Cactus, and Hibiscus plants to the shopping cart, indicated by the \"Add to Cart\" button briefly changing to \"Added!\".\n(00:29-00:34) The user navigates to the \"Cart\" tab, showing the three selected items and the total price, and then briefly views the \"Profile\" tab showing counts for liked plants and cart items.\n(00:37-00:45) After returning to the home screen, the user unlikes the Hibiscus, likes the Snake Plant, and adds the Orchid to their cart.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"Generate a paragraph that summarizes this video. Keep it to 3 to 5 sentences with corresponding timecodes.\" # @param [\"Generate a paragraph that summarizes this video. Keep it to 3 to 5 sentences with corresponding timecodes.\", \"Choose 5 key shots from this video and put them in a table with the timecode, text description of 10 words or less, and a list of objects visible in the scene (with representative emojis).\", \"Generate bullet points for the video. Place each bullet point into an object with the timecode of the bullet point in the video.\"] {\"allow-input\":true}\n",
        "\n",
        "video = user_study_video # @param [\"trailcam_video\", \"pottery_video\", \"post_its_video\", \"user_study_video\"] {\"type\":\"raw\",\"allow-input\":true}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=model_name,\n",
        "    contents=[\n",
        "        video,\n",
        "        prompt,\n",
        "    ]\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEYYemjyKcZ7"
      },
      "source": [
        "# Analyze youtube videos\n",
        "\n",
        "On top of using your own videos you can also ask Gemini to get a video from Youtube and analyze it. He's an example using the keynote from Google IO 2023. Guess what the main theme was?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DP0Dd0hJKvYm",
        "outputId": "c8832bad-10fd-4b8b-b6d1-c7d8a8a0d5a3"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "Okay, here are all the instances where Sundar Pichai says \"AI\" in the provided video, along with timestamps and context:\n\n1.  **0:29** - Context: Setting the stage for the keynote, acknowledging the current focus on AI.\n    > \"As you may have heard, **AI** is having a very busy year.\"\n\n2.  **0:38** - Context: Highlighting Google's long-term commitment and shift towards AI.\n    > \"Seven years into our journey as an **AI**-first company, we are at an exciting inflection point.\"\n\n3.  **0:45** - Context: Describing the goal and potential impact of AI development.\n    > \"We have an opportunity to make **AI** even more helpful for people, for businesses, for communities, for everyone.\"\n\n4.  **0:54** - Context: Referring to Google's ongoing use of AI to improve its products.\n    > \"We've been applying **AI** to make our products radically more helpful for a while.\"\n\n5.  **1:41** - Context: Discussing advancements in Google Workspace features like Smart Compose.\n    > \"Smart Compose led to more advanced writing features powered by **AI**.\"\n\n6.  **3:02** - Context: Explaining the technology behind Google Street View.\n    > \"Since the early days of Street View, **AI** has stitched together billions of panoramic images...\"\n\n7.  **3:15** - Context: Describing the technology used for the Immersive View feature in Google Maps.\n    > \"...Immersive View, which uses **AI** to create a high-fidelity representation of a place...\"\n\n8.  **5:08** - Context: Introducing Google Photos as an example of an AI-enhanced product.\n    > \"Another product made better by **AI** is Google Photos.\"\n\n9.  **5:38** - Context: Explaining how AI enables more powerful photo editing features.\n    > \"**AI** advancements give us more powerful ways to do this.\"\n\n10. **7:40** - Context: Summarizing the product examples shown (Gmail, Photos, Maps).\n    > \"...these are just a few examples of how **AI** can help you in moments that matter.\"\n\n11. **7:47** - Context: Stating the broader goal of leveraging AI across all Google products.\n    > \"...so much more we can do to deliver the full potential of **AI** across the products you know and love.\"\n\n12. **8:22** - Context: Positioning AI as central to Google's mission going forward.\n    > \"Looking ahead, making **AI** helpful for everyone is the most profound way we will advance our mission.\"\n\n13. **8:53** - Context: Highlighting the importance of responsible AI development and deployment.\n    > \"...by building and deploying **AI** responsibly so that everyone can benefit equally.\"\n\n14. **9:02** - Context: Linking the advancement of foundation models to the goal of making AI helpful.\n    > \"Our ability to make **AI** helpful for everyone relies on continuously advancing our foundation models.\"\n\n15. **11:26** - Context: Describing the capabilities of Sec-PaLM, a security-focused AI model.\n    > \"It uses **AI** to better detect malicious scripts...\"\n\n16. **12:14** - Context: Discussing future potential applications of Med-PaLM 2 in healthcare.\n    > \"You can imagine an **AI** collaborator that helps radiologists interpret images...\"\n\n17. **12:46** - Context: Framing PaLM 2 within Google's long-term vision for responsible AI.\n    > \"...latest step in our decade-long journey to bring **AI** in responsible ways to billions of people.\"\n\n18. **14:09** - Context: Emphasizing the parallel investment in AI responsibility alongside model development.\n    > \"As we invest in more advanced models, we're also deeply investing in **AI** responsibility.\"\n\n19. **15:04** - Context: Discussing metadata as a tool for identifying AI-generated content.\n    > \"We'll ensure every one of our **AI**-generated images has that metadata.\"\n\n20. **15:11** - Context: Referring to a future segment specifically on responsible AI.\n    > \"James will talk about our responsible approach to **AI** later.\"\n\n21. **15:30** - Context: Introducing Bard as Google's platform for conversational AI interaction.\n    > \"...our experiment for conversational **AI**.\"",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=model_name,\n",
        "    contents=types.Content(\n",
        "        parts=[\n",
        "            types.Part(text=\"Find all the instances where Sundar says \\\"AI\\\". Provide timestamps and broader context for each instance.\"),\n",
        "            types.Part(\n",
        "                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=ixRanV-rdAQ')\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cizoUEdIYLd0"
      },
      "source": [
        "Once again, you can check the  [live demo on AI Studio](https://aistudio.google.com/starter-apps/video) shows an example on how to postprocess this output. Check the [code of that demo](https://github.com/google-gemini/starter-applets/tree/main/video) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lND4jB6MrsSk"
      },
      "source": [
        "# Next Steps\n",
        "\n",
        "Try with you own videos using the [AI Studio's live demo](https://aistudio.google.com/starter-apps/video) or play with the examples from this notebook (in case you haven't seen, there are other prompts you can try in the dropdowns).\n",
        "\n",
        "For more examples of the Gemini capabilities, check the other guide from the [Cookbook](https://github.com/google-gemini/cookbook/). You'll learn how to use the [Live API](../quickstarts/Get_started_LiveAPI.ipynb), juggle with [multiple tools](../quickstarts/Get_started_LiveAPI_tools.ipynb) or use Gemini 2.0 [spatial understanding](../quickstarts/Spatial_understanding.ipynb) abilities.\n",
        "\n",
        "The [examples](https://github.com/google-gemini/cookbook/tree/main/examples/) folder from the cookbook is also full of nice code samples illustrating creative ways to use Gemini multimodal capabilities and long-context."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Video_understanding.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}